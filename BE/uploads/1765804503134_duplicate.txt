import { ErrorEntity, HttpStatus, LoggerHelper, RequestContext } from "@skillmine-dev/code-utils";
import { DbContext } from '../../../../database/DBContext';
import { z } from "zod";
import * as fs from "fs";
import { CreateOcrProcessingDto } from '../../../workflow-config-srv/services/dto/CreateOcrProcessing.dto'
import { StorageService } from '../../../storage-srv/services/classes/StorageService'
import { SettingsService } from "../../../settings-srv/services/classes/SettingsService"
import { FileProcessTypeEnums } from '../../../storage-srv/services/enums/FileProcessType.enum';
import { DocumentExtractor } from "../../../../utils/DocumentExtractor/DocumentExtractor";
import { LLMHelper } from "../../../../utils/LLMHelper/LLMHelper";
import { ModelHelper } from "../../../../utils/ModelHelper/ModelHelper";
import { RandomNumberGenerator } from "@skillmine-dev/code-utils";
import { ObjectUtil } from "../../../../utils/ObjectUtil/ObjectUtil";
import { SchemaConfigService } from '../../../schema-config-srv/services/classes/SchemaConfigService'
import { CreateDocumentParseDto, CreateDocumentParseWithContentDto, CreateDocumentParseWithDocumentStoreDto } from '../dto/CreateDocumentParsing.dto'
import { UploadedFileService } from '../../../storage-srv/services/classes/UploadedFileService'
import { CreateUploadFileDto } from '../dto/CreateUploadFile.dto'
import { CreateLlmNodeDto } from '../dto/CreateLlmNode.dto'
import axios from "axios";
import { DatabaseConfigService } from "../../../../modules/database-config-srv/services/classes/DatabaseConfigService";
import { CreateDatabasePostProcessingDto } from "../dto/CreateDatabasePostProcessing.dto";
import { IWorkflowExecution } from '../db/WorkflowExecution'
import { DocumentProgressStatusEnum } from '../../../storage-srv/services/enums/DocumentProgressStatus.enum'
import { WorkflowNodeNameEnums } from '../enums/WorkflowNodeName.enum'
import { DocumentCompareDataService } from '../../../document-compare-srv/services/classes/DocumentCompareDataService'
import { CreateDocumentCompareDto } from '../dto/CreateDocumentCompare.dto'
import { CreateTransformDto } from '../dto/CreateTransform.dto'
import { CreateWebhookPostProcessingDto } from "../dto/CreateWebhookPostProcessing.dto"
import { CreateTriggerNodeDto } from '../dto/CreateTriggerNode.dto'


export class WorkflowNodeService {

    private static _instance: WorkflowNodeService;

    static get Instance() {
        if (!this._instance) {
            this._instance = new WorkflowNodeService;
        }
        return this._instance;
    }

    async uploadFile(currentContext: RequestContext, incomming_data: CreateUploadFileDto, execution_data: IWorkflowExecution) {
        let processing_start_time = new Date();
        // let overall_processing_end_time: Date;
        LoggerHelper.Instance.info(currentContext.x_request_id, "Executing uploadFile fn", incomming_data);
        try {
            const { key, name, url, file_type, file_size, file_pages } = incomming_data;
            if (!key) {
                await this.updateProcessingFailure(execution_data, WorkflowNodeNameEnums.FileUploadNode, processing_start_time, { "Upload Node": "Upload node Config is missing" })
                throw new ErrorEntity({
                    http_code: HttpStatus.NOT_FOUND,
                    error_code: "invalid_request",
                    error_description: `File Key is Missing.`
                })
            }
            // Update the status of the node
            await this.updateProcessingSuccess(execution_data, WorkflowNodeNameEnums.FileUploadNode, processing_start_time)
            return incomming_data;
        }
        catch (error) {
            await this.updateProcessingFailure(execution_data, WorkflowNodeNameEnums.FileUploadNode, processing_start_time, error)
            throw error;
        }
    }


    // LLM config Node for the workflow
    // _id,provider,api_key,model_preset
    async llmconfig(currentContext: RequestContext, incomming_data: CreateLlmNodeDto, execution_data: IWorkflowExecution) {
        let processing_start_time = new Date();
        // let overall_processing_end_time: Date;
        LoggerHelper.Instance.info(currentContext.x_request_id, "Executing llmconfig fn", incomming_data);
        try {
            const { api_key, llm_model_name, provider_name } = incomming_data;
            if (!api_key || !llm_model_name) {
                await this.updateProcessingFailure(execution_data, WorkflowNodeNameEnums.LLMProcessingNode, processing_start_time, { "LLM Node": "API Key or LLM model name is missing" })
                throw new ErrorEntity({
                    http_code: HttpStatus.NOT_FOUND,
                    error_code: "invalid_request",
                    error_description: `${!incomming_data.api_key ? "API Key is required" : "Model Name is required"}`
                });
            }
            const url = `https://generativelanguage.googleapis.com/v1beta/models/${llm_model_name}?key=${api_key}`;
            const response = await axios.get(url);
            await this.updateProcessingSuccess(execution_data, WorkflowNodeNameEnums.LLMProcessingNode, processing_start_time)
            if (response.status === 200 && response.data?.name) {
                return Promise.resolve({ valid: true, message: `API key and model '${llm_model_name}' are Configured` });
            }
            else {
                return Promise.resolve({ valid: false, message: `Model '${llm_model_name}' not found ` });
            }
        } catch (error) {
            LoggerHelper.Instance.error(currentContext.x_request_id, "Error downloading file:", incomming_data.api_key);
            await this.updateProcessingFailure(execution_data, WorkflowNodeNameEnums.LLMProcessingNode, processing_start_time, error)
            throw new ErrorEntity({
                http_code: HttpStatus.NOT_FOUND,
                error_code: "LLM_error",
                error_description: `Invalid Api Key: ${incomming_data.api_key}`
            });
        }
    }

    // To get the files from the http node 
    async httpFileUpload(currentContext: RequestContext, incomming_data: CreateTriggerNodeDto, execution_data: IWorkflowExecution) {
        let processing_start_time = new Date();
        try {
            const { file_upload_type, trigger_url, method, url_name, key, name, url, file_type, file_size, file_pages } = incomming_data
            if (!key) {
                await this.updateProcessingFailure(execution_data, WorkflowNodeNameEnums.HTTPNODE, processing_start_time, { "HTTP Node": "File Key is missing" })
                throw new ErrorEntity({
                    http_code: HttpStatus.NOT_FOUND,
                    error_code: "invalid_request",
                    error_description: `File Key is Missing.`
                })
            }
            // Update the status of the node
            await this.updateProcessingSuccess(execution_data, WorkflowNodeNameEnums.HTTPNODE, processing_start_time)
            return { key, name, url, file_type, file_size, file_pages };
        } catch (error) {
            await this.updateProcessingFailure(execution_data, WorkflowNodeNameEnums.HTTPNODE, processing_start_time, error)
            LoggerHelper.Instance.error(currentContext.x_request_id, "Error in httpFileUpload fn", error);
            return Promise.reject(error);
        }
    }

    //To get Document Store's Uploaded Files
    async documentStoreUploadedFiles(currentContext: RequestContext, document_store_id: string, execution_data: IWorkflowExecution) {
        let processing_start_time = new Date();
        try {
            LoggerHelper.Instance.info(currentContext.x_request_id, "documentStoreUploadedFiles fn", { document_store_id });
            if (!document_store_id) {
                await this.updateProcessingFailure(execution_data, WorkflowNodeNameEnums.DocumentStoreNode, processing_start_time, { "Document Store Node": "Document Store Id is Required" })
                throw new ErrorEntity({
                    http_code: HttpStatus.NOT_FOUND,
                    error_code: "DocumentStore_error",
                    error_description: `Invalid Document Store id: ${document_store_id}`
                });
            }
            const query: any = { is_deleted: false, document_store_id: document_store_id };
            const dbContext = await DbContext.getContextByConfig(currentContext.tenant_key);
            const files = await dbContext.UploadedFile.find(query).sort({ createdAt: -1 }).lean();
            await this.updateProcessingSuccess(execution_data, WorkflowNodeNameEnums.DocumentStoreNode, processing_start_time);
            return Promise.resolve(files);
        } catch (error) {
            await this.updateProcessingFailure(execution_data, WorkflowNodeNameEnums.DocumentStoreNode, processing_start_time, error)
            LoggerHelper.Instance.error(currentContext.x_request_id, "Error in documentStoreUploadedFiles fn", error);
            return Promise.reject(error);
        }
    }

    async ocrProcessingForWorkflow(currentContext: RequestContext, incomming_data: CreateOcrProcessingDto, execution_data: IWorkflowExecution) {
        let processing_start_time = new Date();
        // let overall_processing_end_time: Date;
        LoggerHelper.Instance.info(currentContext.x_request_id, "Executing ocrProcessingForWorkflow fn", incomming_data);
        try {
            const { key, name, url, file_type, file_size, file_pages, file_processing_type, provider_name, llm_model_name, api_key } = incomming_data;
            let file_path;
            if (!key) {
                await this.updateProcessingFailure(execution_data, WorkflowNodeNameEnums.OCRProcessingNode, processing_start_time, { "OCR Node": "File Key is missing" })
                throw new ErrorEntity({
                    http_code: HttpStatus.NOT_FOUND,
                    error_code: "invalid_request",
                    error_description: `File Key is Missing.`
                });
            }
            try {
                LoggerHelper.Instance.info(currentContext.x_request_id, "fileDownload fn", { key });
                file_path = await StorageService.Instance.fileDownload(currentContext, key);
            } catch (error) {
                LoggerHelper.Instance.error(currentContext.x_request_id, "Error downloading file:", error);

                await this.updateProcessingFailure(execution_data, WorkflowNodeNameEnums.OCRProcessingNode, processing_start_time, error)

                throw new ErrorEntity({
                    http_code: HttpStatus.INTERNAL_SERVER_ERROR,
                    error_code: "storage_error",
                    error_description: `Error downloading file: ${error}`
                });
            }
            const settings_config = await SettingsService.Instance.getSettings(currentContext);
            if (!settings_config) {
                await this.updateProcessingFailure(execution_data, WorkflowNodeNameEnums.OCRProcessingNode, processing_start_time, { "OCR Node": "Setting Config is missing" })
                throw new ErrorEntity({
                    http_code: HttpStatus.NOT_FOUND,
                    error_code: "invalid_request",
                    error_description: "Settings not found"
                });
            }
            (settings_config.ai_config as any).llm_model_name = "gemini-2.5-flash";
            if (api_key) settings_config.ai_config.api_key = api_key;
            if (llm_model_name) (settings_config.ai_config as any).llm_model_name = llm_model_name;
            // settings_config.ai_config.api_key = api_key;
            // (settings_config.ai_config as any).llm_model_name = llm_model_name;
            let content;
            let summary;
            let visual_processing_start_time = new Date();
            let visual_processing_end_time: Date;
            if (!file_type || !file_path) {
                await this.updateProcessingFailure(execution_data, WorkflowNodeNameEnums.OCRProcessingNode, processing_start_time, { "OCR Node": "file type or key is missing" })
                throw new ErrorEntity({
                    http_code: HttpStatus.NOT_FOUND,
                    error_code: "invalid_request",
                    error_description: `${!file_path ? "File path" : "File Type"} is Missing.`
                });
            }
            try {
                LoggerHelper.Instance.info(currentContext.x_request_id, "Visual processing fn", { key });
                console.log("Start Time for visual processing:", visual_processing_start_time);
                try {
                    if (file_processing_type === FileProcessTypeEnums.AI_OCR) {
                        try {
                            content = await DocumentExtractor.Instance.extractContent({
                                llm_config: settings_config.ai_config,
                                file_path: file_path,
                                mimeType: file_type
                            });
                        } catch (error: any) {
                            // âœ… Check if Gemini is overloaded
                            if (error?.message?.includes("503") || error?.message?.includes("model is overloaded")) {
                                LoggerHelper.Instance.warn(currentContext.x_request_id, "Gemini overloaded, switching to fallback model...", { originalModel: settings_config.ai_config });

                                // Choose a fallback model (any other provider or older version)
                                (settings_config.ai_config as any).llm_model_name = "gemini-2.5-pro";
                                // OR (if OpenAI key configured)
                                // (settings_config.ai_config as any).llm_model_name = "gpt-4o-mini";

                                content = await DocumentExtractor.Instance.extractContent({
                                    llm_config: settings_config.ai_config,
                                    file_path: file_path,
                                    mimeType: file_type
                                });
                            } else {
                                await this.updateProcessingFailure(execution_data, WorkflowNodeNameEnums.OCRProcessingNode, processing_start_time, error)
                                throw error;
                            }
                        }
                    } else {
                        try {
                            content = await DocumentExtractor.Instance.extractTextContent({
                                llm_config: settings_config.ai_config,
                                file_path: file_path,
                                mimeType: file_type
                            });
                        } catch (error: any) {
                            if (error?.message?.includes("503") || error?.message?.includes("model is overloaded")) {
                                LoggerHelper.Instance.warn(currentContext.x_request_id, "Gemini overloaded, switching to fallback model for text extraction...", { originalModel: settings_config.ai_config });
                                (settings_config.ai_config as any).llm_model_name = "gemini-2.5-pro";
                                content = await DocumentExtractor.Instance.extractTextContent({
                                    llm_config: settings_config.ai_config,
                                    file_path: file_path,
                                    mimeType: file_type
                                });
                            } else {
                                await this.updateProcessingFailure(execution_data, WorkflowNodeNameEnums.OCRProcessingNode, processing_start_time, error)
                                throw error;
                            }
                        }
                    }
                } catch (error) {
                    LoggerHelper.Instance.error(currentContext.x_request_id, "All model attempts failed for OCR/Text Extraction", error);
                    await this.updateProcessingFailure(execution_data, WorkflowNodeNameEnums.OCRProcessingNode, processing_start_time, error)
                    throw new ErrorEntity({
                        http_code: HttpStatus.SERVICE_UNAVAILABLE,
                        error_code: "MODEL_OVERLOADED",
                        error_description: "All available OCR models failed due to overload or unavailability."
                    });
                }

                if (!content) {
                    LoggerHelper.Instance.error(currentContext.x_request_id, "Content extraction failed", { key });
                    visual_processing_end_time = new Date();
                } else {
                    LoggerHelper.Instance.info(currentContext.x_request_id, "Content extracted successfully", { key });

                    LoggerHelper.Instance.info(currentContext.x_request_id, "Generating summary...", { key });
                    const textContent = typeof content === 'string' ? content : JSON.stringify(content);

                    const limitedContent = textContent
                        .split(/\s+/)           // split by whitespace
                        .slice(0, 500)          // take first 500 words
                        .join(' ');             // join back into a string

                    const messages: LLMMessageEntity[] = [
                        {
                            role: "system",
                            content: "Your Job is to generate the summary of document with LLM friendly words, dont focus on the documents metadata, extract all necessory information in the document.  Make sure you are generating the summary of 1000 words, do not add your own context. do not ask any questions. and just give the answer because this is getting stored in the database so that we can use it for further processing and searching in the future."
                        },
                        {
                            role: "user",
                            content: `content: ${limitedContent}. `
                        }
                    ];

                    let lowUsageModel = await ModelHelper.Instance.getModel("LOW", "GOOGLE_GENERATIVE_AI", settings_config?.ai_config!);

                    summary = await LLMHelper.Instance.predict({
                        messages: messages,
                        llm_config: settings_config.ai_config!,
                        model_name: lowUsageModel.model
                    });
                    if (!summary) {
                        LoggerHelper.Instance.error(currentContext.x_request_id, "Error generating summary", { key });
                        await this.updateProcessingFailure(execution_data, WorkflowNodeNameEnums.OCRProcessingNode, processing_start_time, { "OCR Node": "OCR Failed or Change the OCR type" })
                        throw new ErrorEntity({
                            http_code: HttpStatus.INTERNAL_SERVER_ERROR,
                            error_code: "SUMMARY_GENERATION_FAILED",
                            error_description: "Summary generation failed"
                        });
                    }
                    LoggerHelper.Instance.info(currentContext.x_request_id, "Summary generated successfully", { key });

                    // Store the extracted content in the uploaded_file
                    const saved_s3_config = settings_config?.file_storage_config;
                    if (!saved_s3_config) {
                        await this.updateProcessingFailure(execution_data, WorkflowNodeNameEnums.OCRProcessingNode, processing_start_time, { "OCR Node": "S3 config is missing" })
                        throw new ErrorEntity({
                            http_code: HttpStatus.BAD_REQUEST,
                            error_code: "s3_config_not_found",
                            error_description: "S3 configuration not found for the workflow."
                        });
                    }

                    if (!saved_s3_config.bucket_name || !saved_s3_config.region || !saved_s3_config.access_key_id || !saved_s3_config.secret_access_key) {
                        throw new ErrorEntity({
                            http_code: HttpStatus.BAD_REQUEST,
                            error_code: "s3_config_invalid",
                            error_description: "Invalid S3 configuration for the workflow."
                        });
                    }

                    const { bucket_name, region, access_key_id, secret_access_key, endpoint } = saved_s3_config;

                    // Instead of: uploaded_file.file_content = content;

                    const jsonFileName = `${RandomNumberGenerator.getUniqueId()}_content.json`;
                    let jsonContent = { data: content };

                    await StorageService.Instance.uploadJsonToS3({
                        accessKeyId: access_key_id,
                        secretAccessKey: secret_access_key,
                        region: region,
                        endpoint: endpoint,
                        bucketName: bucket_name,
                        filePath: jsonFileName,
                        data: jsonContent,
                        contentType: "application/json"
                    });

                    // Store reference to S3 file path instead of raw content
                    visual_processing_end_time = new Date();
                    console.log("End Time for visual processing:", visual_processing_end_time);
                    await this.updateProcessingSuccess(execution_data, WorkflowNodeNameEnums.OCRProcessingNode, processing_start_time)
                    return Promise.resolve({ summary, "content": jsonFileName })
                }
            }
            catch (error) {
                await this.updateProcessingFailure(execution_data, WorkflowNodeNameEnums.OCRProcessingNode, processing_start_time, error)
                LoggerHelper.Instance.error(currentContext.x_request_id, "Error extracting content:", error);
                throw error; // Re-throw
            }

        } catch (error) {
            await this.updateProcessingFailure(execution_data, WorkflowNodeNameEnums.OCRProcessingNode, processing_start_time, error)
            LoggerHelper.Instance.error(
                currentContext.x_request_id,
                "ocrProcessingForWorkflow fn Failed",
                error
            );
            throw error;
        }
    }

    // Parsing for the Workflow
    async processDocumentParsing(currentContext: RequestContext, incomming_data: CreateDocumentParseDto, execution_data: IWorkflowExecution) {
        let processing_start_time = new Date();
        // let overall_processing_end_time: Date;
        LoggerHelper.Instance.info(currentContext.x_request_id, "Executing processDocumentParsing fn", incomming_data);
        const { summary, api_key, llm_model_name, provider_name, schema_id, content, instruction, document_store_id, file_ids, parse_all_files } = incomming_data;
        try {
            const settings_config = await SettingsService.Instance.getSettings(currentContext);
            if (!settings_config) {
                await this.updateProcessingFailure(execution_data, WorkflowNodeNameEnums.DocumentParsingNode, processing_start_time, { "Document Parsing Node": "Setting Config or LLM Node is missing" })
                throw new ErrorEntity({
                    http_code: HttpStatus.NOT_FOUND,
                    error_code: "invalid_request",
                    error_description: "Settings not found"
                });
            }
            (settings_config.ai_config as any).llm_model_name = "gemini-2.5-flash";
            if (api_key) settings_config.ai_config.api_key = api_key;
            if (llm_model_name) (settings_config.ai_config as any).llm_model_name = llm_model_name;

            let parsed_document: any;
            if (document_store_id && file_ids) {
                let payload = {
                    "api_key": settings_config.ai_config.api_key,
                    "llm_model_name": (settings_config.ai_config as any).llm_model_name,
                    "schema_id": schema_id,
                    document_store_id,
                    "file_ids": file_ids || [],
                    instruction,
                    "parse_all_files": parse_all_files ?? false
                }
                parsed_document = await this.parsingwithDocumentStore(currentContext, payload, execution_data)
            }
            if (content && summary) {
                let payload = {
                    "api_key": settings_config.ai_config.api_key,
                    "llm_model_name": (settings_config.ai_config as any).llm_model_name,
                    "schema_id": schema_id,
                    content,
                    summary,
                    instruction
                }
                parsed_document = await this.parsingWithContent(currentContext, payload, execution_data)
            }
            await this.updateProcessingSuccess(execution_data, WorkflowNodeNameEnums.DocumentParsingNode, processing_start_time)
            return Promise.resolve(parsed_document);
        }
        catch (error) {
            await this.updateProcessingFailure(execution_data, WorkflowNodeNameEnums.DocumentParsingNode, processing_start_time, error)
            LoggerHelper.Instance.error('', "Error in processDocumentParsing fn", error);
            return Promise.reject(error);
        }
    }

    // Node can be parsed if the document store and file IDs is present
    async parsingwithDocumentStore(currentContext: RequestContext, incomming_data: CreateDocumentParseWithDocumentStoreDto, execution_data: IWorkflowExecution) {
        let processing_start_time = new Date();
        // let overall_processing_end_time: Date;
        LoggerHelper.Instance.info('', "parseDocument fn", { incomming_data });
        try {
            const { api_key, llm_model_name, provider_name, schema_id, instruction, document_store_id, file_ids, parse_all_files } = incomming_data;

            let document_parsing_start_time = new Date();
            // Early input validations
            if (!document_store_id) {
                await this.updateProcessingFailure(execution_data, WorkflowNodeNameEnums.DocumentParsingNode, processing_start_time, { "Document Parsing Node": "Document Store Id is missing" })
                throw new ErrorEntity({
                    http_code: HttpStatus.INTERNAL_SERVER_ERROR,
                    error_code: "DOCUMENT_STORE_ID_NOT_FOUND",
                    error_description: `Document Store ID is required for document parsing.`
                });
            }

            // Get the file IDs by document store
            let uploaded_files = [] as any;
            uploaded_files = await UploadedFileService.Instance.getUploadedFilesByDocumentStoreId_filesIds(document_store_id, incomming_data.file_ids);
            incomming_data.file_ids = uploaded_files.map(file => file._id);

            if (!incomming_data.file_ids || incomming_data.file_ids.length === 0 || uploaded_files.length === 0) {
                await this.updateProcessingFailure(execution_data, WorkflowNodeNameEnums.DocumentParsingNode, processing_start_time, { "Document Parsing Node": "No Files Found" })
                throw new ErrorEntity({
                    http_code: HttpStatus.INTERNAL_SERVER_ERROR,
                    error_code: "NO_FILES_FOUND",
                    error_description: `No files found for the provided Document Store or file IDs are not valid.`
                });
            }

            // Parse mode flag - true: parse all files, false: parse only new files
            let shouldParseAllFiles = parse_all_files ?? false;

            let parsed_files_data = execution_data.parsed_files || { parsed_file_ids: [], combined_result: null, schema_id: null };
            let already_parsed_file_ids = parsed_files_data.parsed_file_ids || [];
            let previous_schema_id = parsed_files_data.schema_id || null;
            let new_file_ids: string[];

            // Check if schema_id has changed - if yes, parse all files again
            if (!shouldParseAllFiles && previous_schema_id && schema_id && previous_schema_id !== schema_id) {
                LoggerHelper.Instance.info('', "Schema ID changed - parsing all files again", {
                    previous_schema_id,
                    new_schema_id: schema_id
                });
                shouldParseAllFiles = true;
            }

            if (shouldParseAllFiles) {
                // Mode: Parse all files again (ignore duplicate check)
                LoggerHelper.Instance.info('', "Parse mode: Parse ALL files", {
                    total_files: incomming_data.file_ids.length,
                    mode: "parse_all_files",
                    reason: previous_schema_id !== schema_id ? "schema_changed" : "parse_all_files_flag"
                });
                new_file_ids = incomming_data.file_ids;
                // Reset the tracking - we'll replace with new data
                already_parsed_file_ids = [];
            } else {
                // Mode: Parse only new files and combine (duplicate check enabled)
                // Filter out already parsed files to get only new files
                new_file_ids = incomming_data.file_ids.filter(file_id => !already_parsed_file_ids.includes(file_id));

                LoggerHelper.Instance.info('', "Parse mode: Parse NEW files only and combine", {
                    total_files: incomming_data.file_ids.length,
                    already_parsed: already_parsed_file_ids.length,
                    new_files_to_parse: new_file_ids.length,
                    new_file_ids,
                    mode: "parse_new_only"
                });

                // If no new files to parse, return the existing combined result
                if (new_file_ids.length === 0) {
                    LoggerHelper.Instance.info('', "No new files to parse, returning existing parsed data", parsed_files_data);
                    await this.updateProcessingSuccess(execution_data, WorkflowNodeNameEnums.DocumentParsingNode, processing_start_time);
                    return Promise.resolve(parsed_files_data.combined_result || {});
                }
            }

            // Update file_ids to only include files to be parsed
            incomming_data.file_ids = new_file_ids;
            LoggerHelper.Instance.info('', "ParsingwithDocumentStore fn", incomming_data);

            // Retrieve setting
            const setting_config = await SettingsService.Instance.getSettings(currentContext);
            if (!setting_config) {
                await this.updateProcessingFailure(execution_data, WorkflowNodeNameEnums.DocumentParsingNode, processing_start_time, { "Document Parsing Node": "Setting Config or LLM Node is missing" })
                throw new Error(`Setting is not found`);
            }
            if (api_key) setting_config.ai_config.api_key = api_key;
            if (llm_model_name) (setting_config.ai_config as any).llm_model_name = llm_model_name;

            let llm_config: any = setting_config?.ai_config;

            if (!llm_config) {
                await this.updateProcessingFailure(execution_data, WorkflowNodeNameEnums.DocumentParsingNode, processing_start_time, { "Document Parsing Node": "Setting Config or LLM Node is missing" })
                throw new ErrorEntity({
                    http_code: HttpStatus.INTERNAL_SERVER_ERROR,
                    error_code: "LLM_CONFIG_NOT_FOUND",
                    error_description: `LLM configuration is required for document parsing.`
                });
            }

            const file_metadata_list = await UploadedFileService.Instance.getUploadedFileByFileIDs(file_ids!);
            if (!file_metadata_list || file_metadata_list.length === 0) {
                await this.updateProcessingFailure(execution_data, WorkflowNodeNameEnums.DocumentParsingNode, processing_start_time, { "Document Parsing Node": "No Content Found in Document Store" })
                throw new ErrorEntity({
                    http_code: HttpStatus.INTERNAL_SERVER_ERROR,
                    error_code: "FILE_METADATA_NOT_FOUND",
                    error_description: `File metadata is required for document parsing.`
                });
            }

            // Start parsing
            console.log("Start Time for Document Parsing:", document_parsing_start_time);
            const content_list: string[] = [];

            const saved_s3_config = await StorageService.Instance.getConfig(
                currentContext
            );

            if (!saved_s3_config) {
                await this.updateProcessingFailure(execution_data, WorkflowNodeNameEnums.DocumentParsingNode, processing_start_time, { "Document Parsing Node": "S3 is missing" })
                throw new ErrorEntity({
                    http_code: HttpStatus.BAD_REQUEST,
                    error_code: "s3_config_not_found",
                    error_description: "S3 configuration not found for the workflow."
                });
            }

            if (!saved_s3_config.bucketName || !saved_s3_config.region || !saved_s3_config.accessKeyId || !saved_s3_config.secretAccessKey) {
                await this.updateProcessingFailure(execution_data, WorkflowNodeNameEnums.DocumentParsingNode, processing_start_time, { "Document Parsing Node": "S3 is missing" })
                throw new ErrorEntity({
                    http_code: HttpStatus.BAD_REQUEST,
                    error_code: "s3_config_invalid",
                    error_description: "Invalid S3 configuration for the workflow."
                });
            }

            const { bucketName, region, accessKeyId, secretAccessKey, endpoint } = saved_s3_config;
            for (const file_metadata of file_metadata_list) {
                if (!file_metadata || !file_metadata.file_content) {
                    LoggerHelper.Instance.warn(document_store_id, "File metadata or content is missing", { file_metadata });
                    continue;
                }

                // if file_metadata.content includes .json at end
                if (file_metadata.file_content.endsWith('_content.json')) {

                    const fileContentBuffer = await StorageService.Instance.readFileFromS3({
                        accessKeyId: accessKeyId,
                        secretAccessKey: secretAccessKey,
                        region: region,
                        endpoint: endpoint,
                        bucketName: bucketName,
                        filePath: file_metadata.file_content,
                    });
                    let originalData;
                    try {
                        // Convert buffer to string
                        const fileString = fileContentBuffer.toString("utf-8");

                        // If it's JSON stored, parse it
                        originalData = JSON.parse(fileString);
                    } catch (error) {
                        await this.updateProcessingFailure(execution_data, WorkflowNodeNameEnums.DocumentParsingNode, processing_start_time, error)
                        throw new Error(`Invalid JSON or text format in file: ${file_metadata.file_content}`);
                    }

                    const data = originalData?.data;
                    content_list.push(typeof data === 'string' ? data : JSON.stringify(data));
                } else {
                    content_list.push(file_metadata.file_content);
                }
            }
            let payload = {
                api_key,
                llm_model_name,
                "schema_id": schema_id || undefined,
                "content": content_list.join(','),
                "instruction": instruction ?? "",
            }
            let new_parsed_result = await this.parsingWithContent(currentContext, payload, execution_data);

            // Combine previous parsed result with new parsed result
            // BUT: If we're reparsing all files (due to schema change or parse_all_files flag),
            // don't use previous results - start fresh
            const previous_result = shouldParseAllFiles ? {} : (parsed_files_data.combined_result || {});
            let combined_result: any;

            // Merge logic: If both are objects, merge them; if arrays, concatenate
            if (Array.isArray(previous_result) && Array.isArray(new_parsed_result)) {
                combined_result = [...previous_result, ...new_parsed_result];
            } else if (typeof previous_result === 'object' && typeof new_parsed_result === 'object') {
                combined_result = { ...previous_result, ...new_parsed_result };
            } else if (Object.keys(previous_result).length === 0) {
                // If no previous result, use new result
                combined_result = new_parsed_result;
            } else {
                // Default: wrap both in an array
                combined_result = [previous_result, new_parsed_result];
            }

            // Update execution_data with combined result and all parsed file IDs
            const all_parsed_file_ids = [...already_parsed_file_ids, ...new_file_ids];
            execution_data.parsed_files = {
                parsed_file_ids: all_parsed_file_ids,
                combined_result: combined_result,
                schema_id: schema_id,
                last_updated: new Date()
            };

            // Save the execution data
            await execution_data.save();

            LoggerHelper.Instance.info('', "Combined parsing results successfully", {
                previous_parsed_count: already_parsed_file_ids.length,
                new_parsed_count: new_file_ids.length,
                total_parsed_count: all_parsed_file_ids.length
            });

            return Promise.resolve(combined_result);
        }
        catch (error) {
            await this.updateProcessingFailure(execution_data, WorkflowNodeNameEnums.DocumentParsingNode, processing_start_time, error)
            LoggerHelper.Instance.error('', "Error in ParsingwithDocumentStore fn", error);
            return Promise.reject(error);
        }
    }
    // Node can be parsed only the content and summary is present
    async parsingWithContent(currentContext: RequestContext, incomming_data: CreateDocumentParseWithContentDto, execution_data: IWorkflowExecution) {
        let processing_start_time = new Date();
        // let overall_processing_end_time: Date;

        const { api_key, llm_model_name, provider_name, schema_id, content, summary, instruction } = incomming_data;
        let document_parsing_start_time = new Date();
        if (!content) {
            await this.updateProcessingFailure(execution_data, WorkflowNodeNameEnums.DocumentParsingNode, processing_start_time, { "Document Parsing Node": "Content is missing" })
            throw new ErrorEntity({
                http_code: HttpStatus.INTERNAL_SERVER_ERROR,
                error_code: "CONTENT_NOT_FOUND",
                error_description: `Content is required for document parsing.`
            });
        }
        try {
            const content_list: string[] = [];

            const saved_s3_config = await StorageService.Instance.getConfig(
                currentContext
            );

            if (!saved_s3_config) {
                await this.updateProcessingFailure(execution_data, WorkflowNodeNameEnums.DocumentParsingNode, processing_start_time, { "Document Parsing Node": "S3 Config is missing" })
                throw new ErrorEntity({
                    http_code: HttpStatus.BAD_REQUEST,
                    error_code: "s3_config_not_found",
                    error_description: "S3 configuration not found for the workflow."
                });
            }

            if (!saved_s3_config.bucketName || !saved_s3_config.region || !saved_s3_config.accessKeyId || !saved_s3_config.secretAccessKey) {
                await this.updateProcessingFailure(execution_data, WorkflowNodeNameEnums.DocumentParsingNode, processing_start_time, { "Document Parsing Node": "S3 config is missing" })
                throw new ErrorEntity({
                    http_code: HttpStatus.BAD_REQUEST,
                    error_code: "s3_config_invalid",
                    error_description: "Invalid S3 configuration for the workflow."
                });
            }

            const { bucketName, region, accessKeyId, secretAccessKey, endpoint } = saved_s3_config;

            if (content.endsWith('_content.json')) {
                const fileContentBuffer = await StorageService.Instance.readFileFromS3({
                    accessKeyId: accessKeyId,
                    secretAccessKey: secretAccessKey,
                    region: region,
                    endpoint: endpoint,
                    bucketName: bucketName,
                    filePath: content,
                });
                let originalData;
                try {
                    // Convert buffer to string
                    const fileString = fileContentBuffer.toString("utf-8");

                    // If it's JSON stored, parse it
                    originalData = JSON.parse(fileString);
                } catch (error) {
                    await this.updateProcessingFailure(execution_data, WorkflowNodeNameEnums.DocumentParsingNode, processing_start_time, error)
                    throw new Error(`Invalid JSON or text format in file: ${content}`);
                }
                const data = originalData?.data;
                content_list.push(typeof data === 'string' ? data : JSON.stringify(data));
            }
            else {
                content_list.push(content);
            }

            if (content_list.length === 0) {
                await this.updateProcessingFailure(execution_data, WorkflowNodeNameEnums.DocumentParsingNode, processing_start_time, { "Document Parsing Node": "No content available to parse from the provided files." })
                throw new ErrorEntity({
                    http_code: HttpStatus.INTERNAL_SERVER_ERROR,
                    error_code: "NO_CONTENT_TO_PARSE",
                    error_description: `No content available to parse from the provided files.`
                });
            }

            let saved_schema: z.ZodTypeAny | null = null;
            let generated_schema: any = null;

            const settings_config = await SettingsService.Instance.getSettings(currentContext);
            if (!settings_config) {
                await this.updateProcessingFailure(execution_data, WorkflowNodeNameEnums.DocumentParsingNode, processing_start_time, { "Document Parsing Node": "Settings not found" })
                throw new ErrorEntity({
                    http_code: HttpStatus.NOT_FOUND,
                    error_code: "invalid_request",
                    error_description: "Settings not found"
                });
            }
            if (api_key) settings_config.ai_config.api_key = api_key;
            if (llm_model_name) (settings_config.ai_config as any).llm_model_name = llm_model_name;
            const llm_config = settings_config.ai_config
            if (schema_id) {
                const schema_config = await SchemaConfigService.Instance.getSchemaConfigByIdInternal(schema_id);
                const plain_schema = schema_config.output_schema_structure;
                saved_schema = ObjectUtil.convertJsonSchemaToZod(plain_schema);
            } else {
                // If no schema_id is provided, we can create a new schema based on the content
                saved_schema = null;

                const new_schema = await LLMHelper.Instance.generateSchemaFromDocument({
                    documentContent: content_list.join("\n"),
                    llm_config,
                });

                generated_schema = new_schema;
                saved_schema = new_schema ? ObjectUtil.convertJsonSchemaToZod(new_schema) : null;
            }

            const parsed_document = await LLMHelper.Instance.parseDocument({
                content: content_list.join("\n"),
                schema: saved_schema || undefined,
                llm_config: llm_config,
                instruction: instruction ?? ""
            });

            // Update status to DONE
            let document_parse_end_time = new Date();
            let document_parse_duration = Math.floor((document_parse_end_time.getTime() - document_parsing_start_time.getTime()) / 1000); // in seconds

            console.log("End Time for Document Parsing:", document_parse_end_time);
            console.log("Document parsing time taken: ", document_parse_duration, " seconds");
            LoggerHelper.Instance.info(content, "Document parsing completed successfully", { parsed_document });
            return Promise.resolve(parsed_document);

        } catch (error) {
            await this.updateProcessingFailure(execution_data, WorkflowNodeNameEnums.DocumentParsingNode, processing_start_time, error)
            LoggerHelper.Instance.error(currentContext.x_request_id, "ParsingWithContent fn Failed", error);
            throw error;
        }
    }

    //Compare document 
    async processCompareDocument(currentContext: RequestContext, incomming_request: CreateDocumentCompareDto, execution_data: IWorkflowExecution) {
        LoggerHelper.Instance.info('', "processCompareDocument fn", incomming_request);
        let processing_start_time = new Date();
        const { document_store_id, source_file_id, target_file_id, provider_name, llm_model_name, api_key, instruction } = incomming_request;
        if (!document_store_id) {
            await this.updateProcessingFailure(execution_data, WorkflowNodeNameEnums.DocumentCompareNode, processing_start_time, { "Compare Node": `Document Store ID is required for document parsing` })
            throw new ErrorEntity({
                http_code: HttpStatus.INTERNAL_SERVER_ERROR,
                error_code: "DOCUMENT_STORE_ID_NOT_FOUND",
                error_description: `Document Store ID is required for document parsing.`
            });
        }
        if (!source_file_id || !target_file_id) {
            await this.updateProcessingFailure(execution_data, WorkflowNodeNameEnums.DocumentCompareNode, processing_start_time, { "Compare Node": `${!source_file_id ? "Source File Id" : "Target File Id"} for document comparison.` })
            throw new ErrorEntity({
                http_code: HttpStatus.BAD_REQUEST,
                error_code: "INVALID_INPUT",
                error_description: `${!source_file_id ? "Source File Id" : "Target File Id"} for document comparison.`
            });
        }
        const source_file_metadata: any = await UploadedFileService.Instance.getUploadedFileById(currentContext, source_file_id);
        const target_file_metadata: any = await UploadedFileService.Instance.getUploadedFileById(currentContext, target_file_id);

        try {
            // *** MODIFIED: Get file paths for document comparison
            let source_file_path: string;
            let target_file_path: string;
            try {
                source_file_path = await StorageService.Instance.fileDownload(currentContext, source_file_metadata.key);
                target_file_path = await StorageService.Instance.fileDownload(currentContext, target_file_metadata.key);
            } catch (error) {
                LoggerHelper.Instance.error('', "Error downloading files:", error);
                await this.updateProcessingFailure(execution_data, WorkflowNodeNameEnums.DocumentCompareNode, processing_start_time, error)
                throw error; // Re-throw or handle the exception appropriately
            }

            const settings_config = await SettingsService.Instance.getSettings(currentContext);
            if (!settings_config) {
                await this.updateProcessingFailure(execution_data, WorkflowNodeNameEnums.DocumentCompareNode, processing_start_time, { "Compare Node": "Settings not found" })
                throw new ErrorEntity({
                    http_code: HttpStatus.NOT_FOUND,
                    error_code: "invalid_request",
                    error_description: "Settings not found"
                });
            }
            (settings_config.ai_config as any).llm_model_name = "gemini-2.5-flash";
            if (api_key) settings_config.ai_config.api_key = api_key;
            if (llm_model_name) (settings_config.ai_config as any).llm_model_name = llm_model_name;

            // *** MODIFIED: Call DocumentExtractor.Instance.compareDocuments directly ***
            let comparison_result;
            let disliked_differences = await DocumentCompareDataService.Instance.getDocumentCompareDislikedDataByDocumentStoreID(currentContext, document_store_id);
            try {
                comparison_result = await DocumentExtractor.Instance.compareDocuments({
                    llm_config: settings_config?.ai_config,
                    templateFilePath: source_file_path,
                    filledFilePath: target_file_path,
                    document_store_id,
                    disliked_differences,
                    "instructions": instruction
                });
            } catch (error) {
                await this.updateProcessingFailure(execution_data, WorkflowNodeNameEnums.DocumentCompareNode, processing_start_time, error)
                LoggerHelper.Instance.error('', "Error comparing documents:", error);
                throw error; // Re-throw or handle the exception appropriately
            }
            finally {
                if (source_file_path && fs.existsSync(source_file_path)) {
                    fs.unlinkSync(source_file_path);
                }
                if (target_file_path && fs.existsSync(target_file_path)) {
                    fs.unlinkSync(target_file_path);
                }
            }
            if (comparison_result && comparison_result.differences && Array.isArray(comparison_result.differences)) {
                const updatedDifferences = comparison_result.differences.map((difference: any) => ({
                    ...difference,
                    disliked: false
                }));
                comparison_result.differences = updatedDifferences;
            }
            // if comparison_result.differences is not an array or is undefined, we can initialize it to all the objects with disliked set to false
            if (!comparison_result.differences || !Array.isArray(comparison_result.differences)) {
                comparison_result.differences = [];
            }

            // Update the document compare data status to COMPLETED
            let document_compare_end_time = new Date();
            let document_comparing_start_time = processing_start_time || new Date();
            let document_compare_duration = Math.floor((document_compare_end_time.getTime() - document_comparing_start_time.getTime()) / 1000); // in seconds

            console.log("End Time for Document Comparison:", document_compare_end_time);
            console.log("Duration for Document Comparison:", document_compare_duration, "seconds");
            LoggerHelper.Instance.info('', "Document comparison completed successfully", { comparison_result });
            await this.updateProcessingSuccess(execution_data, WorkflowNodeNameEnums.DocumentCompareNode, processing_start_time)
            return Promise.resolve(comparison_result);
        } catch (error) {
            await this.updateProcessingFailure(execution_data, WorkflowNodeNameEnums.DocumentCompareNode, processing_start_time, error)
            LoggerHelper.Instance.error('', "Error in processCompareDocument fn", error);
            return Promise.reject(error);
        }
    }

    //Transform Data 
    async transformData(currentContext: RequestContext, incomming_request: CreateTransformDto, execution_data: IWorkflowExecution) {
        let processing_start_time = new Date();
        LoggerHelper.Instance.info(currentContext.x_request_id, "Executing transformData fn", incomming_request);
        try {
            const { provided_data, mappings } = incomming_request
            const result: Record<string, any> = {};
            if (!provided_data) {
                await this.updateProcessingFailure(execution_data, WorkflowNodeNameEnums.TransformNode, processing_start_time, { "Transform Node": "No datas found to transform" })
                throw new ErrorEntity({
                    http_code: HttpStatus.NOT_FOUND,
                    error_code: "invalid_request",
                    error_description: `No datas found to transform`,
                })
            }
            if (typeof provided_data !== "object") {
                await this.updateProcessingFailure(execution_data, WorkflowNodeNameEnums.TransformNode, processing_start_time, { "Transform Node": "Data should be in object" })
                throw new ErrorEntity({
                    http_code: HttpStatus.BAD_REQUEST,
                    error_code: "invalid_formal",
                    error_description: `Data should be in object`,
                })
            }
            if (!Array.isArray(mappings)) {
                await this.updateProcessingFailure(execution_data, WorkflowNodeNameEnums.TransformNode, processing_start_time, { "Transform Node": "Mapping should be in Array" })
                throw new ErrorEntity({
                    http_code: HttpStatus.BAD_REQUEST,
                    error_code: "invalid_formal",
                    error_description: `Mapping should be in Array`,
                })
            }
            for (const map of mappings) {
                if (!map.source || !map.target) {
                    LoggerHelper.Instance.warn(currentContext.x_request_id, `Skipping invalid mapping entry`, map);
                    continue;
                }
                const sourceKeys = map.source.split(".");
                const targetKeys = map.target.split('.');
                const value = this.getValueByPath(provided_data, sourceKeys) || "";
                this.setValueByPath(result, targetKeys, value);
            }
            await this.updateProcessingSuccess(execution_data, WorkflowNodeNameEnums.TransformNode, processing_start_time)
            LoggerHelper.Instance.info(currentContext.x_request_id, "TransformNode Execution Completed", { result });
            return result;
        } catch (error) {
            await this.updateProcessingFailure(execution_data, WorkflowNodeNameEnums.TransformNode, processing_start_time, error)
            LoggerHelper.Instance.error(currentContext.x_request_id, "handleDatabasePostProcessing fn failed", { message: error?.message });
            throw error;
        }
    }

    // To get the nested Value
    private getValueByPath(provided_data: Record<string, any>, keys: string[]) {
        try {
            return keys.reduce((acc, key) => (acc ? acc[key] : ""), provided_data)
        } catch (error) {
            return "";
        }

    }

    /** Safely set nested value (creates intermediate objects) */
    private setValueByPath(provided_data: Record<string, any>, keys: string[], value: any): void {
        keys.reduce((acc, key, index) => {
            if (index === keys.length - 1) {
                acc[key] = value;
            } else {
                if (!acc[key] || typeof acc[key] !== "object") {
                    acc[key] = {};
                }
            }
            return acc[key];
        }, provided_data);
    }

    /**
     * Handles inserting workflow output into target database 
     */
    async handleDatabasePostProcessing(currentContext: RequestContext, incomingData: CreateDatabasePostProcessingDto, execution_data: IWorkflowExecution) {
        let processing_start_time = new Date();
        // let overall_processing_end_time: Date;
        LoggerHelper.Instance.info(currentContext.x_request_id, "Executing handleDatabasePostProcessing fn", incomingData);

        try {
            const { connection_string, database_name, collection_name, results } = incomingData;

            // Step 1: Validation (handled by class-validator automatically)
            if (!connection_string || !database_name || !collection_name) {
                await this.updateProcessingFailure(execution_data, WorkflowNodeNameEnums.DatabaseNode, processing_start_time, { "DataBase Node": "Database connection details are missing or invalid." })
                throw new ErrorEntity({
                    http_code: HttpStatus.BAD_REQUEST,
                    error_code: "missing_database_details",
                    error_description: "Database connection details are missing or invalid.",
                });
            }

            LoggerHelper.Instance.info(currentContext.x_request_id, "Database details validated successfully", {
                database_name,
                collection_name,
            });

            // Step 2: Prepare data
            const dataToInsert = {
                result: results || {},
                created_at: new Date(),
            };

            LoggerHelper.Instance.info(currentContext.x_request_id, "Preparing data for insertion", dataToInsert);

            // Step 3: Insert into Database
            try {
                await DatabaseConfigService.Instance.insertFileDocumentForWorkflow(
                    currentContext,
                    connection_string,
                    database_name,
                    collection_name,
                    dataToInsert
                );
            } catch (dbError) {
                LoggerHelper.Instance.error(currentContext.x_request_id, "Database insertion failed", dbError);
                await this.updateProcessingFailure(execution_data, WorkflowNodeNameEnums.DatabaseNode, processing_start_time, { "DataBase Node": `Error inserting into ${database_name}.${collection_name}: ${dbError.message}` })
                throw new ErrorEntity({
                    http_code: HttpStatus.INTERNAL_SERVER_ERROR,
                    error_code: "database_insertion_failed",
                    error_description: `Error inserting into ${database_name}.${collection_name}: ${dbError.message}`,
                });
            }
            await this.updateProcessingSuccess(execution_data, WorkflowNodeNameEnums.DatabaseNode, processing_start_time)
            //Step 4: Return structured response
            return Promise.resolve({
                success: true,
                inserted_into: `${database_name}.${collection_name}`,
                data: dataToInsert,
            });

        } catch (error) {
            await this.updateProcessingFailure(execution_data, WorkflowNodeNameEnums.DatabaseNode, processing_start_time, error)
            LoggerHelper.Instance.error(currentContext.x_request_id, "handleDatabasePostProcessing fn failed", { message: error?.message, stack: error?.stack });
            if (!(error instanceof ErrorEntity)) {
                throw new ErrorEntity({
                    http_code: HttpStatus.INTERNAL_SERVER_ERROR,
                    error_code: "database_postprocessing_failed",
                    error_description: error?.message || "Unexpected error occurred during database post-processing.",
                });
            }
            throw error;
        }
    }

    // Handles inserting workflow output into target webhook
    async handleWebhookPostProcessing(currentContext: RequestContext, incomingData: CreateWebhookPostProcessingDto, execution_data: IWorkflowExecution) {
        let processing_start_time = new Date();
        try {
            if (!incomingData?.webhook_url) {
                throw new ErrorEntity({
                    http_code: HttpStatus.BAD_REQUEST,
                    error_code: "Webhook_url_not_found",
                    error_description: "Webhook Url is required",
                });
            }

            const webhook_url = incomingData?.webhook_url;
            const method = (incomingData?.method || 'post').toLowerCase();
            const webhook_key = incomingData?.webhook_key;
            const webhook_value = incomingData?.webhook_value;
            const api_key_format = (incomingData?.api_key_format || 'headers').toLowerCase();
            const workflow_result = incomingData?.results || {};
            if (!webhook_url) {
                await this.updateProcessingFailure(execution_data, WorkflowNodeNameEnums.WebhookNode, processing_start_time, { WebhookNode: "Webhook URL is required" })
                throw new ErrorEntity({
                    http_code: HttpStatus.BAD_REQUEST,
                    error_code: "webhook_url_missing",
                    error_description: "Webhook URL is required"
                });
            }
            let headers: any = { 'Content-Type': 'application/json' };
            let finalUrl = webhook_url;
            let bodyPayload: any = Array.isArray(workflow_result) ? [...workflow_result] : { ...(workflow_result || {}) };
            if (webhook_key && webhook_value) {
                if (api_key_format === 'headers') {
                    headers[webhook_key] = webhook_value;
                } else if (api_key_format === 'query') {
                    const queryParam = `${encodeURIComponent(webhook_key)}=${encodeURIComponent(webhook_value)}`;
                    finalUrl += finalUrl.includes('?') ? `&${queryParam}` : `?${queryParam}`;
                } else if (api_key_format === 'body') {
                    if (typeof bodyPayload === 'object') {
                        (bodyPayload as any)[webhook_key] = webhook_value;
                    } else {
                        // if it's not object, send wrapped payload
                        bodyPayload = {
                            payload: bodyPayload,
                            [webhook_key]: webhook_value
                        };
                    }
                }
            }

            let response;
            if (method === 'post') {
                response = await axios.post(finalUrl, bodyPayload, { headers });
            } else if (method === 'get') {
                response = await axios.get(finalUrl, { headers, params: bodyPayload });
            } else {
                await this.updateProcessingFailure(execution_data, WorkflowNodeNameEnums.WebhookNode, processing_start_time, { WebhookNode: "Unsupported webhook method: ${method}" })
                throw new ErrorEntity({
                    http_code: HttpStatus.BAD_REQUEST,
                    error_code: "unsupported_webhook_method",
                    error_description: `Unsupported webhook method: ${method}`
                });
            }
            await this.updateProcessingSuccess(execution_data, WorkflowNodeNameEnums.WebhookNode, processing_start_time)
            LoggerHelper.Instance.info(currentContext.x_request_id, "Webhook called successfully", { url: finalUrl, method, status: response?.status });
            return response.data || { "success": true, "message": "Webhook successfull" };
        } catch (error) {
            await this.updateProcessingFailure(execution_data, WorkflowNodeNameEnums.WebhookNode, processing_start_time, error)
            LoggerHelper.Instance.error(currentContext.x_request_id, "handleWebhookPostProcessing fn failed", error);
            throw error;
        }
    }

    async updateProcessingFailure(execution_data: any, nodeName: WorkflowNodeNameEnums, processing_start_time: Date, error: any) {
        try {
            const overall_processing_end_time = new Date();
            const overall_processing_start_time: Date = execution_data?.overall_proccessing?.start_time || new Date();

            const overallTimeTaken = (overall_processing_end_time.getTime() - overall_processing_start_time.getTime()) / 1000;
            const node_status_overall_time = (overall_processing_end_time.getTime() - processing_start_time.getTime()) / 1000;
            // Update overall processing
            execution_data.overall_proccessing_status = DocumentProgressStatusEnum.FAILED;

            execution_data.overall_proccessing = {
                start_time: overall_processing_start_time,
                end_time: overall_processing_end_time,
                time_taken: overallTimeTaken,
                status: DocumentProgressStatusEnum.FAILED,
                error: error.message || error || "Unkown Error",
            };

            // Update node status
            const node_status = {
                node_name: nodeName,
                status: DocumentProgressStatusEnum.FAILED,
                start_time: processing_start_time,
                end_time: overall_processing_end_time,
                time_taken: node_status_overall_time,
                error: error.message || error || "Unkown Error",
            };

            // if (execution_data.node_status.length === 0) {
            //     execution_data.node_status = [];
            //     execution_data.node_status.push(node_status);
            // }

            execution_data.node_status.push(node_status);

            await execution_data.save();

            return execution_data;
        }
        catch (error) {
            throw error
        }
    }

    //Updating the status
    async updateProcessingSuccess(execution_data: any, nodeName: WorkflowNodeNameEnums, processing_start_time: Date) {
        try {
            const overall_processing_end_time = new Date();
            const overallTimeTaken =
                (overall_processing_end_time.getTime() -
                    processing_start_time.getTime()) /
                1000;

            // Update node status
            const node_status = {
                node_name: nodeName,
                status: DocumentProgressStatusEnum.DONE,
                start_time: processing_start_time,
                end_time: overall_processing_end_time,
                time_taken: overallTimeTaken,
            };

            // if (execution_data.node_status.length === 0) {
            //     execution_data.node_status = [];
            //     execution_data.node_status.push(node_status);
            // }

            execution_data.node_status.push(node_status);
            await execution_data.save();
        }
        catch (error) {
            throw error;
        }
    }

}


